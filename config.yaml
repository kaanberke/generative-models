# Checkpointing parameters
Checkpoint:
  dirpath: "checkpoints"
  save_top_k: 1
  filename: "model_{epoch:02d}_{val_generator_loss:.2f}"
  monitor: "val_generator_loss"
  mode: "min"
  verbose: true

# Trainer related parameters
Trainer:
  data_dir: "data/processed/thumbnails128x128"
  train_val_test_split: [0.8, 0.1, 0.1]
  lr: 3e-4
  factor: 0.2
  patience: 5
  verbose: True
  max_epochs: 50
  batch_size: 8
  image_size: 128
  accelerator: "mps" # cuda/cpu/mps
  num_sanity_val_steps: 0
  monitor: "val_generator_loss"
  mode: "min"

# Optimizer Params
Optimizer:
  name: "Adam"
  params:
    lr: 0.0003

Scheduler:
  name: "ReduceLROnPlateau"
  params:
    factor: 0.2
    patience: 5
    verbose: True
    mode: "min"


# VAE model-specific parameters
VAE:
  input_dimension: [3, 128, 128]  # [Channels, Width, Height]
  latent_dimension: 20

  # Encoder Convolutional Layers: [out_channels, kernel_size, stride, padding]
  encoder_conv_layers:
    - [32, 3, 2, 1]
    - [64, 3, 2, 1]
    - [128, 3, 2, 1]
    - [256, 3, 2, 1]

  # Decoder Deconvolutional Layers:
  # [out_channels, kernel_size, stride, padding]
  decoder_deconv_layers:
    - [128, 4, 2, 1]
    - [64, 4, 2, 1]
    - [32, 4, 2, 1]
    - [16, 4, 2, 1]
    - [8, 4, 2, 1]
    - [4, 4, 2, 1]
    - [3, 4, 2, 1]


GAN:
  latent_dim: 100  # Size of the random noise vector for the generator

  generator_layers:
    - 100  # Input: Latent dimension
    - 256  # Hidden layer
    - 512  # Hidden layer
    - 1024 # Hidden layer
    - 49152  # Output: Flattened image dimension (e.g., 28x28 for MNIST)

  discriminator_layers:
    - 49152  # Input: Flattened image dimension
    - 512  # Hidden layer
    - 256  # Hidden layer
    - 1    # Output: Single value representing probability image is real

  Optimizer:
    name: Adam
    params:
      lr: 0.0002
      betas: [0.5, 0.999]

  Scheduler:
    name: CosineAnnealingLR
    params:
      # Number of epochs before the learning rate is reset
      T_max: 10

DCGAN:
    Generator:
        latent_dim: 100  # Size of the random noise vector fed into the generator
        initial_size: [100, 1, 1]  # Initial spatial size and channels for the generator
        layers:  # ConvTranspose2d layers for the generator
            - [100, 512, 4, 2, 1]          # [input_channels, output_channels, kernel_size, stride, padding]
            - [512, 512, 4, 2, 1]          # [input_channels, output_channels, kernel_size, stride, padding]
            - [512, 256, 4, 2, 1]
            - [256, 256, 4, 2, 1]
            - [256, 128, 4, 2, 1]
            - [128, 64, 4, 2, 1]
            - [64, 3, 4, 2, 1]
        final_activation: tanh  # Activation for the final layer of the generator
        intermediate_activations: relu  # Activation for the intermediate layers of the generator

    Discriminator:
        initial_channels: 3  # Initial channels of the input image (3 for RGB images)
        layers:  # Conv2d layers for the discriminator
            - [3, 64, 4, 2, 1]          # [input_channels, output_channels, kernel_size, stride, padding]
            - [64, 128, 4, 2, 1] 
            - [128, 256, 4, 2, 1]
            - [256, 512, 4, 2, 1]
            - [512, 1024, 3, 2, 1]
            - [1024, 1, 4, 1, 0]
        final_activation: sigmoid  # Activation for the final layer of the discriminator
        intermediate_activations: leaky_relu  # Activation for the intermediate layers of the discriminator

    Optimizer:
        name: Adam  # Optimizer type
        params:  # Parameters for the optimizer
            lr: 0.0002
            betas: [0.5, 0.999]

    Scheduler:
        name: StepLR  # Learning rate scheduler type
        params:  # Parameters for the learning rate scheduler
            step_size: 50
            gamma: 0.5


CycleGAN:
  Generator:
    initial_size: [100, 1, 1]  # Latent dimension and initial size before convolutions.
    layers:
      - [100, 512, 4, 1, 0]          # [input_channels, output_channels, kernel_size, stride, padding]
      - [512, 256, 4, 2, 1]
      - [256, 128, 4, 2, 1]
      - [128, 64, 4, 2, 1]
      - [64, 3, 4, 2, 1]            # Output channels should match the image channels (e.g., 3 for RGB images)
    intermediate_activations: leaky_relu
    final_activation: tanh

  Discriminator:
    layers:
      - [3, 64, 4, 2, 1]
      - [64, 128, 4, 2, 1]
      - [128, 256, 4, 2, 1]
      - [256, 512, 4, 1, 0]
      - [512, 1, 4, 1, 0]
    intermediate_activations: leaky_relu
    final_activation: sigmoid

  Loss:
    lambda_cycle: 10.0   # Weight for cycle consistency loss
    lambda_identity: 0.5  # Weight for identity loss

  Optimizer:
    name: Adam
    params:
      lr: 0.0002
      betas: [0.5, 0.999]

  Scheduler:
    name: LambdaLR
    params:
      lr_lambda: [Your Lambda Function for Learning Rate Adjustment]
