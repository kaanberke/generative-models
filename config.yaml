# Checkpointing parameters
Checkpoint:
  dirpath: "checkpoints"
  save_top_k: 1
  filename: "model_{epoch:02d}_{val_generator_loss:.2f}"
  monitor: "val_generator_loss"
  mode: "min"
  verbose: true

# Trainer related parameters
Trainer:
  data_dir: "data/processed/thumbnails128x128"
  train_val_test_split: [0.8, 0.1, 0.1]
  lr: 3e-4
  factor: 0.2
  patience: 5
  verbose: True
  max_epochs: 50
  batch_size: 2
  image_size: 128
  accelerator: "mps" # cuda/cpu/mps
  num_sanity_val_steps: 0
  monitor: "val_generator_loss"
  mode: "min"

# Optimizer Params
Optimizer:
  name: "Adam"
  params:
    lr: 0.0003

Scheduler:
  name: "ReduceLROnPlateau"
  params:
    factor: 0.2
    patience: 5
    verbose: True
    mode: "min"


VAE:
  input_dimension: [3, 128, 128]
  latent_dimension: 20

  encoder_conv_layers:
    - [32, 3, 2, 1]
    - [64, 3, 2, 1]
    - [128, 3, 2, 1]
    - [256, 3, 2, 1]

  decoder_deconv_layers:
    - [128, 4, 2, 1]
    - [64, 4, 2, 1]
    - [32, 4, 2, 1]
    - [16, 4, 2, 1]
    - [8, 4, 2, 1]
    - [4, 4, 2, 1]
    - [3, 4, 2, 1]


GAN:
  latent_dim: 100

  generator_layers:
    - 100
    - 256
    - 512
    - 1024
    - 49152

  discriminator_layers:
    - 49152
    - 512
    - 256
    - 1

  Optimizer:
    name: Adam
    params:
      lr: 0.0002
      betas: [0.5, 0.999]

  Scheduler:
    name: CosineAnnealingLR
    params:
      T_max: 10

DCGAN:
    Generator:
        latent_dim: 100
        initial_size: [100, 1, 1]
        layers:
            - [100, 512, 4, 2, 1]
            - [512, 512, 4, 2, 1]
            - [512, 256, 4, 2, 1]
            - [256, 256, 4, 2, 1]
            - [256, 128, 4, 2, 1]
            - [128, 64, 4, 2, 1]
            - [64, 3, 4, 2, 1]
        final_activation: tanh
        intermediate_activations: relu

    Discriminator:
        initial_channels: 3
        layers:
            - [3, 64, 4, 2, 1]
            - [64, 128, 4, 2, 1] 
            - [128, 256, 4, 2, 1]
            - [256, 512, 4, 2, 1]
            - [512, 1024, 3, 2, 1]
            - [1024, 1, 4, 1, 0]
        final_activation: sigmoid
        intermediate_activations: leaky_relu

    Optimizer:
        name: Adam
        params:
            lr: 0.0002
            betas: [0.5, 0.999]

    Scheduler:
        name: StepLR
        params:
            step_size: 50
            gamma: 0.5


CycleGAN:
  Generator:
    initial_size: [100, 1, 1]
    layers:
      - [100, 512, 4, 1, 0]
      - [512, 256, 4, 2, 1]
      - [256, 128, 4, 2, 1]
      - [128, 64, 4, 2, 1]
      - [64, 3, 4, 2, 1]
    intermediate_activations: leaky_relu
    final_activation: tanh

  Discriminator:
    layers:
      - [3, 64, 4, 2, 1]
      - [64, 128, 4, 2, 1]
      - [128, 256, 4, 2, 1]
      - [256, 512, 4, 1, 0]
      - [512, 1, 4, 1, 0]
    intermediate_activations: leaky_relu
    final_activation: sigmoid

  Loss:
    lambda_cycle: 10.0
    lambda_identity: 0.5

  Optimizer:
    name: Adam
    params:
      lr: 0.0002
      betas: [0.5, 0.999]

  Scheduler:
    name: LambdaLR
    params:
      lr_lambda: [Your Lambda Function for Learning Rate Adjustment]


WGAN:
  latent_dim: 100
  initial_size: [100, 1, 1]
  
  generator_layers:
    - [100, 256, 4, 2, 1]
    - [256, 512, 4, 2, 1]
    - [512, 1024, 4, 2, 1]
    - [1024, 49152, 4, 2, 1]

  critic_layers:
    - [49152, 1024, 4, 2, 1]
    - [1024, 512, 4, 2, 1]
    - [512, 256, 4, 2, 1]
    - [256, 1, 4, 2, 1]

  intermediate_activations: relu
  final_activation: tanh

  Optimizer:
    name: Adam
    params:
      lr: 0.0002
      betas: [0.5, 0.999]

  Scheduler:
    name: CosineAnnealingLR
    params:
      T_max: 10
  
StyleGAN:
  resolution: 128  # Target resolution of generated images.
  latent_dim: 512  # Dimensionality of the latent space.

  mapping_layers:  # Layers for the mapping network from latent space to intermediate latent space.
    - [512, 512]
    - [512, 512]
    - [512, 512]
    - [512, 512]
    - [512, 512]
    - [512, 512]
    - [512, 512]
    - [512, 512]

  synthesis_layers:  # Parameters for the synthesis network to generate images.
    - [512, 512, 4, 1, 0]
    - [512, 512, 4, 2, 1]
    - [512, 512, 4, 2, 1]
    - [512, 256, 4, 2, 1]
    - [256, 128, 4, 2, 1]
    - [128, 3, 4, 2, 1]  

  # Additional parameters specific to StyleGAN architecture.
  truncation_psi: 0.7
  truncation_cutoff: 8
  style_mixing_prob: 0.9  # Probability of mixing styles during training

  Optimizer:
    name: Adam
    params:
      lr: 0.001
      betas: [0.0, 0.99]
      eps: 0.00000001

  Scheduler:
    name: CosineAnnealingLR
    params:
      T_max: 10
  
  Loss:
    r1_gamma: 10.0  # R1 regularization weight

  Training:
    batch_size: 16
    epochs: 100
    gradient_accumulation_steps: 1  # How many steps to accumulate gradients before updating weights
